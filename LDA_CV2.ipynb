{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac604ecc-adee-43e2-9a6a-067258a078b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14dfe74a-caf4-4004-9239-55da58c93291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (1.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (1.20.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3324a181-da7a-4b46-9dee-4eec9cb518da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# libraries for visualization\n",
    "import pyLDAvis\n",
    "#import pyLDAvis.gensim\n",
    "\n",
    "#https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a155e27-1af7-41dc-9d25-bab35a858680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.2.4-cp39-cp39-win_amd64.whl (11.3 MB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.15-cp39-cp39-win_amd64.whl (1.0 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
      "Collecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from spacy) (21.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp39-cp39-win_amd64.whl (112 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp39-cp39-win_amd64.whl (36 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (3.0.2)\n",
      "Collecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (2.25.1)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp39-cp39-win_amd64.whl (21 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.7-cp39-cp39-win_amd64.whl (6.6 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.3-cp39-cp39-win_amd64.whl (448 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: click<8.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from spacy) (8.0.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp39-cp39-win_amd64.whl (1.9 MB)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from click<8.1.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.5.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->spacy) (2.0.1)\n",
      "Installing collected packages: murmurhash, cymem, catalogue, wasabi, typer, srsly, smart-open, pydantic, preshed, blis, thinc, spacy-loggers, spacy-legacy, pathy, langcodes, spacy\n",
      "Successfully installed blis-0.7.7 catalogue-2.0.7 cymem-2.0.6 langcodes-3.3.0 murmurhash-1.0.6 pathy-0.6.1 preshed-3.0.6 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 wasabi-0.9.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34b60d38-5e56-47e0-8dfe-bb28e6017665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp39-cp39-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.20.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Collecting Cython==0.29.23\n",
      "  Downloading Cython-0.29.23-cp39-cp39-win_amd64.whl (1.7 MB)\n",
      "Installing collected packages: Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.24\n",
      "    Uninstalling Cython-0.29.24:\n",
      "      Successfully uninstalled Cython-0.29.24\n",
      "Successfully installed Cython-0.29.23 gensim-4.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25c9053a-6963-428f-a2ac-c25086b35ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis\n",
      "  Using cached pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Installing backend dependencies: started\n",
      "  Installing backend dependencies: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Collecting funcy\n",
      "  Using cached funcy-1.17-py2.py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: scikit-learn in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (1.20.3)\n",
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: setuptools in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis) (58.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (3.0.2)\n",
      "Requirement already satisfied: pandas>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (1.3.3)\n",
      "Requirement already satisfied: future in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: scipy in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis) (1.7.1)\n",
      "Requirement already satisfied: gensim in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis) (4.1.2)\n",
      "Requirement already satisfied: joblib in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: numexpr in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=1.2.0->pyLDAvis) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.16.0)\n",
      "Requirement already satisfied: Cython==0.29.23 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from gensim->pyLDAvis) (0.29.23)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Building wheels for collected packages: pyLDAvis, sklearn\n",
      "  Building wheel for pyLDAvis (PEP 517): started\n",
      "  Building wheel for pyLDAvis (PEP 517): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136900 sha256=eb5c6e1dabeb169b83f16970bb2b9ca2fc81085f4ac95720f089529d76d50681\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\57\\a4\\86\\d10c6c2e0bf149fbc0afb0aa5a6528ac35b30a133a0270c477\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Created wheel for sklearn: filename=sklearn-0.0-py2.py3-none-any.whl size=1309 sha256=ccaf3b12e45c51a8e4f742bc95676d3afc51f38cb1a0a945584ee7df5cfa93a9\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\e4\\7b\\98\\b6466d71b8d738a0c547008b9eb39bf8676d1ff6ca4b22af1c\n",
      "Successfully built pyLDAvis sklearn\n",
      "Installing collected packages: sklearn, funcy, pyLDAvis\n",
      "Successfully installed funcy-1.17 pyLDAvis-3.3.1 sklearn-0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1ed4b7ea-b55d-4ab1-9688-5f45b487c08c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda_installation\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3444: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Sanctions/chord/highlighted_sentiments_survey.csv', error_bad_lines=False);\n",
    "data_text = data[['CV2']]\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2aa4110e-e3d0-497f-92ad-3a2510ab9893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150\n",
      "                                                 CV2\n",
      "0  Store i was working in had to close due the re...\n",
      "1  I could not find a job now because everything ...\n",
      "2  Lectures are cancelled and exams postponed, it...\n",
      "3  During the last month I am not working in a bi...\n",
      "4  I have not been able to go to work which made ...\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))\n",
    "print(documents[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "f651fa9b-afe1-4f6d-9897-698d26afbca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "b090b4cc-245f-404f-ba49-5ca868a45572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "go\n"
     ]
    }
   ],
   "source": [
    "print(WordNetLemmatizer().lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9642efed-99fc-4d34-9934-8681b29e2237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>caresses</td>\n",
       "      <td>caress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flies</td>\n",
       "      <td>fli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dies</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mules</td>\n",
       "      <td>mule</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>denied</td>\n",
       "      <td>deni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>died</td>\n",
       "      <td>die</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>humbled</td>\n",
       "      <td>humbl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sized</td>\n",
       "      <td>size</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>meeting</td>\n",
       "      <td>meet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>stating</td>\n",
       "      <td>state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>siezing</td>\n",
       "      <td>siez</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>itemization</td>\n",
       "      <td>item</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>sensational</td>\n",
       "      <td>sensat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>traditional</td>\n",
       "      <td>tradit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>reference</td>\n",
       "      <td>refer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>colonizer</td>\n",
       "      <td>colon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>plotted</td>\n",
       "      <td>plot</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   original word stemmed\n",
       "0       caresses  caress\n",
       "1          flies     fli\n",
       "2           dies     die\n",
       "3          mules    mule\n",
       "4         denied    deni\n",
       "5           died     die\n",
       "6         agreed    agre\n",
       "7          owned     own\n",
       "8        humbled   humbl\n",
       "9          sized    size\n",
       "10       meeting    meet\n",
       "11       stating   state\n",
       "12       siezing    siez\n",
       "13   itemization    item\n",
       "14   sensational  sensat\n",
       "15   traditional  tradit\n",
       "16     reference   refer\n",
       "17     colonizer   colon\n",
       "18       plotted    plot"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "           'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "           'traditional', 'reference', 'colonizer','plotted']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed97a11-b857-41ba-a932-b22d42dd7114",
   "metadata": {},
   "source": [
    " # Data Pre-processing\n",
    "   - Tokenization: Split the text into sentences and the sentences into words. \n",
    "   - Lowercase the words and remove punctuation.\n",
    "   - Words that have fewer than 3 characters are removed.\n",
    "   -  All stopwords are removed.\n",
    "   - Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
    "   -  Words are stemmed — words are reduced to their root form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "caae9e48-0cfc-4d84-8f4d-15d02b30a943",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2d41b-eded-4e84-8a8b-a7c958226985",
   "metadata": {},
   "source": [
    "# whithout stemming only lemmatization then run the following code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29f1d55-437b-4fd0-8e45-9a3e4943a87f",
   "metadata": {},
   "source": [
    "# whithout stemming\n",
    "\n",
    "```python\n",
    "def lemmatize_stemming(text):\n",
    "    return WordNetLemmatizer().lemmatize(text, pos='v')\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "fcd7a37c-d97a-457a-8fe8-fe4ffd004b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs = documents['CV2'].fillna('').astype(str).map(preprocess)\n",
    "# fillna('').astype(str)  --> helps to ignore the empty rows (Nan) in CV2 column.\n",
    "\n",
    "\n",
    "# processed_docs = documents['CV2'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "159b1bd7-bfc5-4a29-9ba2-13d36e7eab17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not working so that means that I am not earning money'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " documents['CV2'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "ecf50d12-f173-4967-8282-6c04657811c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mostly the only real change is that I am working from home and not seeing anyone in person -- I am taking more calls and reaching out to friends via zoom for socialization, and going to \"virtual\" events at times. I also am making an effort to do things that \"fill me up\", like hobbies and exercise and reading. I am cooking all my food at home, and freezing leftovers and produce to avoid having to go to the stores, which are out of many items.'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " documents['CV2'][600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "49a9fc9e-2e32-467b-bd8c-f250471a94b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                       [store, work, close, restrict]\n",
       "1         [close, student, univers, close, home, shut]\n",
       "2    [lectur, cancel, exam, postpon, difficult, pla...\n",
       "3    [month, work, compani, anymor, current, situat...\n",
       "4    [abl, work, negat, impact, financi, situat, un...\n",
       "5    [easi, order, groceri, shop, deliv, groceri, f...\n",
       "6                         [famili, stay, home, stress]\n",
       "7                                    [stay, quaranten]\n",
       "8      [hasn, affect, home, offic, wife, miss, honest]\n",
       "9                                         [hour, work]\n",
       "Name: CV2, dtype: object"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d33da5-49f7-44c9-8b0a-197cde054eb7",
   "metadata": {},
   "source": [
    "# Bag of words on the CV2 column\n",
    "\n",
    "Create a dictionary from ‘processed_docs’ containing the number of times a word appears in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "6bbc5bd4-74a0-4261-b0e7-42834ac00719",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fc47aac5-3c76-4d06-a4d1-42ae9c19ec71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 close\n",
      "1 restrict\n",
      "2 store\n",
      "3 work\n",
      "4 home\n",
      "5 shut\n",
      "6 student\n",
      "7 univers\n",
      "8 cancel\n",
      "9 difficult\n",
      "10 exam\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4fb7b5-a2eb-4058-bf79-84f4b92e4074",
   "metadata": {},
   "source": [
    "# Gensim filter_extremes\n",
    "## Filter out tokens that appear in\n",
    "\n",
    "- less than 15 documents (absolute number) or\n",
    "- more than 0.5 documents (fraction of total corpus size, not absolute number). \n",
    "- after the above two steps, keep only the first 100000 most frequent tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "57cb571c-35f2-46a5-9f40-584527e0342f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538908f1-cc92-4459-a868-269970cf03d2",
   "metadata": {},
   "source": [
    "# Gensim doc2bow\n",
    "\n",
    "For each document we create a dictionary reporting how many\n",
    "words and how many times those words appear.\n",
    "Save this to ‘bow_corpus’, then check our selected document earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "05ada350-0b9a-4283-8f82-ebd7c508b89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (53, 1), (120, 1)]"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "bow_corpus[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ae974dc4-8872-4376-91b1-6d65ed421755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 1), (53, 1), (120, 1)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "5611ced2-c28b-46c9-b7aa-544626c2328a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am not working so that means that I am not earning money'"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " documents['CV2'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c39af016-dc0a-4102-9770-cd9a91b9180b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_sample =  documents['CV2'][200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "7c19eaf9-4d60-44f5-8e23-690048a0f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['I', 'am', 'not', 'working', 'so', 'that', 'means', 'that', 'I', 'am', 'not', 'earning', 'money']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['work', 'mean', 'earn', 'money']\n"
     ]
    }
   ],
   "source": [
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a77605-cdc2-4a74-af7e-7bfa57011e10",
   "metadata": {},
   "source": [
    "# Select a document to preview after preprocessing.\n",
    "## Preview Bag Of Words for our sample preprocessed document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f5293fe0-88bb-4385-a50f-a65a0a160894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 3 (\"work\") appears 1 time.\n",
      "Word 53 (\"money\") appears 1 time.\n",
      "Word 120 (\"mean\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_200 = bow_corpus[200]\n",
    "\n",
    "for i in range(len(bow_doc_200)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_200[i][0], \n",
    "                                                     dictionary[bow_doc_200[i][0]], \n",
    "                                                     bow_doc_200[i][1]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "c0d3b1c3-2aea-4e37-8989-9997eec5f79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 2 (\"store\") appears 1 time.\n",
      "Word 3 (\"work\") appears 1 time.\n",
      "Word 4 (\"home\") appears 2 time.\n",
      "Word 12 (\"chang\") appears 1 time.\n",
      "Word 22 (\"thing\") appears 1 time.\n",
      "Word 30 (\"social\") appears 1 time.\n",
      "Word 34 (\"like\") appears 1 time.\n",
      "Word 55 (\"time\") appears 1 time.\n",
      "Word 62 (\"person\") appears 1 time.\n",
      "Word 66 (\"friend\") appears 1 time.\n",
      "Word 68 (\"have\") appears 1 time.\n",
      "Word 74 (\"go\") appears 1 time.\n",
      "Word 78 (\"take\") appears 1 time.\n",
      "Word 140 (\"food\") appears 1 time.\n",
      "Word 153 (\"make\") appears 1 time.\n",
      "Word 191 (\"exercis\") appears 1 time.\n",
      "Word 194 (\"item\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "bow_doc_600 = bow_corpus[600]\n",
    "\n",
    "for i in range(len(bow_doc_600)):\n",
    "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_600[i][0], \n",
    "                                                     dictionary[bow_doc_600[i][0]], \n",
    "                                                     bow_doc_600[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6aa6aa4-08e1-4c46-99d3-54f7b946ccb4",
   "metadata": {},
   "source": [
    "#  TF-IDF\n",
    "TF-IDF (term frequency-inverse document frequency) is a statistical measure that evaluates how relevant a word is to a document in a collection of documents.\n",
    "\n",
    "This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "So, words that are common in every document, such as `this, what, and if`, rank low even though they may appear many times, since they don’t mean much to that document in particular.\n",
    "\n",
    "However, if the word \"Bug\" appears many times in a document, while not appearing many times in others, it probably means that it’s very relevant.\n",
    "\n",
    "- Create `tf-idf model` object using `models.TfidfModel` on ‘bow_corpus’ and save it to ‘tfidf’, then apply transformation to the entire corpus and call it ‘corpus_tfidf’. Finally we preview TF-IDF scores for our first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9bdacb28-6e71-4555-811d-0f0a567af238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(bow_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b2f2d852-e055-4e00-a0b4-f2af934934ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tfidf = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "4a53573d-5598-472e-a18d-f115a130fff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4744877646555355),\n",
      " (1, 0.6553798865530638),\n",
      " (2, 0.5665436263439183),\n",
      " (3, 0.15609895881449373)]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "for doc in corpus_tfidf:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014dffc9-2af9-4600-b381-9fdbda2537b0",
   "metadata": {},
   "source": [
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    "\n",
    "- The `term frequency` TF of a word in a document. There are several ways of calculating this frequency, with the simplest being a raw count of instances a word appears in a document. Then, there are ways to adjust the frequency, by length of a document, or by the raw frequency of the most frequent word in a document.\n",
    "- The `inverse document frequency` IDF  of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is. This metric can be calculated by taking the total number of documents, dividing it by the number of documents that contain a word, and calculating the logarithm.\n",
    "    - So, if the word is very common and appears in many documents, this number will approach 0. Otherwise, it will approach 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04088349-1949-4054-b98e-be80fa034e8b",
   "metadata": {},
   "source": [
    "# Running LDA using Bag of Words\n",
    "Train our LDA model using `gensim.models.LdaMulticore` and save it to ‘lda_model’"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "71b4dabf-0fd7-4d67-b63e-30d202f38456",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 10\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics= num_topics, id2word=dictionary, passes=2, workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d71b50-c70b-40bf-8a9a-757a4d0ab5b6",
   "metadata": {},
   "source": [
    "# Print the Keyword in the 10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c751b4-3f02-45b3-b355-ef628b00bea6",
   "metadata": {},
   "source": [
    "For each topic, we will explore the words occuring in that topic and its relative weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "e2145d47-38a8-4116-b858-ddc2fc1dbca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.082*\"work\" + 0.050*\"home\" + 0.048*\"affect\" + 0.020*\"go\" + 0.019*\"social\" + 0.018*\"health\" + 0.017*\"life\" + 0.017*\"school\" + 0.016*\"covid\" + 0.014*\"impact\"\n",
      "Topic: 1 \n",
      "Words: 0.079*\"home\" + 0.069*\"work\" + 0.055*\"time\" + 0.023*\"week\" + 0.021*\"groceri\" + 0.019*\"person\" + 0.018*\"virus\" + 0.018*\"hous\" + 0.018*\"stay\" + 0.015*\"coronavirus\"\n",
      "Topic: 2 \n",
      "Words: 0.043*\"famili\" + 0.043*\"work\" + 0.039*\"home\" + 0.034*\"feel\" + 0.026*\"stay\" + 0.026*\"affect\" + 0.022*\"hous\" + 0.019*\"friend\" + 0.017*\"peopl\" + 0.016*\"get\"\n",
      "Topic: 3 \n",
      "Words: 0.086*\"home\" + 0.057*\"work\" + 0.028*\"cancel\" + 0.023*\"plan\" + 0.022*\"affect\" + 0.020*\"march\" + 0.019*\"month\" + 0.019*\"infect\" + 0.018*\"stay\" + 0.018*\"trip\"\n",
      "Topic: 4 \n",
      "Words: 0.108*\"work\" + 0.061*\"home\" + 0.022*\"time\" + 0.019*\"covid\" + 0.017*\"care\" + 0.017*\"peopl\" + 0.015*\"reduc\" + 0.015*\"hour\" + 0.014*\"famili\" + 0.014*\"school\"\n",
      "Topic: 5 \n",
      "Words: 0.103*\"work\" + 0.065*\"home\" + 0.023*\"groceri\" + 0.021*\"school\" + 0.017*\"shop\" + 0.017*\"quarantin\" + 0.016*\"close\" + 0.016*\"time\" + 0.016*\"money\" + 0.015*\"week\"\n",
      "Topic: 6 \n",
      "Words: 0.074*\"famili\" + 0.051*\"worri\" + 0.035*\"friend\" + 0.029*\"stress\" + 0.029*\"sick\" + 0.024*\"meet\" + 0.023*\"home\" + 0.022*\"hand\" + 0.022*\"get\" + 0.018*\"anxieti\"\n",
      "Topic: 7 \n",
      "Words: 0.073*\"home\" + 0.058*\"work\" + 0.042*\"famili\" + 0.037*\"incom\" + 0.030*\"school\" + 0.029*\"close\" + 0.024*\"quarantin\" + 0.021*\"econom\" + 0.020*\"affect\" + 0.020*\"stress\"\n",
      "Topic: 8 \n",
      "Words: 0.059*\"work\" + 0.040*\"home\" + 0.036*\"get\" + 0.029*\"like\" + 0.023*\"sick\" + 0.021*\"furlough\" + 0.021*\"virus\" + 0.020*\"affect\" + 0.019*\"person\" + 0.019*\"current\"\n",
      "Topic: 9 \n",
      "Words: 0.069*\"home\" + 0.061*\"work\" + 0.041*\"affect\" + 0.034*\"limit\" + 0.027*\"stay\" + 0.024*\"life\" + 0.022*\"person\" + 0.021*\"covid\" + 0.020*\"food\" + 0.016*\"peopl\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "cffd9346-58fa-475e-b0a0-64c8f83f3561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.101*\"home\" + 0.088*\"work\" + 0.044*\"stay\" + 0.026*\"affect\" + '\n",
      "  '0.025*\"family\" + 0.025*\"income\" + 0.024*\"time\" + 0.021*\"limit\" + '\n",
      "  '0.019*\"store\" + 0.016*\"reduce\"'),\n",
      " (1,\n",
      "  '0.072*\"work\" + 0.050*\"home\" + 0.034*\"affect\" + 0.031*\"family\" + '\n",
      "  '0.021*\"time\" + 0.017*\"virus\" + 0.015*\"health\" + 0.015*\"need\" + '\n",
      "  '0.014*\"school\" + 0.014*\"covid\"'),\n",
      " (2,\n",
      "  '0.117*\"work\" + 0.059*\"home\" + 0.029*\"company\" + 0.024*\"income\" + '\n",
      "  '0.022*\"time\" + 0.022*\"friends\" + 0.021*\"furlough\" + 0.020*\"online\" + '\n",
      "  '0.019*\"cancel\" + 0.019*\"family\"'),\n",
      " (3,\n",
      "  '0.075*\"work\" + 0.064*\"home\" + 0.054*\"time\" + 0.042*\"cause\" + 0.032*\"lose\" + '\n",
      "  '0.023*\"money\" + 0.021*\"worry\" + 0.019*\"stress\" + 0.017*\"situation\" + '\n",
      "  '0.017*\"school\"'),\n",
      " (4,\n",
      "  '0.066*\"affect\" + 0.042*\"work\" + 0.041*\"coronavirus\" + 0.035*\"home\" + '\n",
      "  '0.034*\"people\" + 0.029*\"hand\" + 0.028*\"virus\" + 0.021*\"fear\" + '\n",
      "  '0.019*\"weeks\" + 0.018*\"life\"'),\n",
      " (5,\n",
      "  '0.114*\"work\" + 0.088*\"home\" + 0.034*\"close\" + 0.030*\"school\" + 0.029*\"stay\" '\n",
      "  '+ 0.020*\"social\" + 0.013*\"hours\" + 0.013*\"affect\" + 0.013*\"week\" + '\n",
      "  '0.013*\"shop\"'),\n",
      " (6,\n",
      "  '0.076*\"work\" + 0.064*\"home\" + 0.035*\"able\" + 0.033*\"worry\" + 0.019*\"time\" + '\n",
      "  '0.019*\"people\" + 0.018*\"children\" + 0.017*\"care\" + 0.016*\"groceries\" + '\n",
      "  '0.016*\"need\"'),\n",
      " (7,\n",
      "  '0.048*\"work\" + 0.044*\"affect\" + 0.041*\"covid\" + 0.037*\"life\" + 0.031*\"time\" '\n",
      "  '+ 0.030*\"house\" + 0.022*\"feel\" + 0.021*\"hours\" + 0.020*\"daily\" + '\n",
      "  '0.020*\"home\"'),\n",
      " (8,\n",
      "  '0.043*\"work\" + 0.043*\"home\" + 0.039*\"get\" + 0.038*\"family\" + 0.026*\"sick\" + '\n",
      "  '0.026*\"limit\" + 0.023*\"life\" + 0.021*\"outside\" + 0.020*\"shop\" + 0.017*\"go\"'),\n",
      " (9,\n",
      "  '0.095*\"work\" + 0.074*\"home\" + 0.033*\"quarantine\" + 0.024*\"feel\" + '\n",
      "  '0.021*\"food\" + 0.019*\"health\" + 0.018*\"close\" + 0.017*\"able\" + '\n",
      "  '0.017*\"store\" + 0.016*\"live\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2689fd58-c447-4144-82b5-8665e64091e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyLDAvis==2.1.2\n",
      "  Downloading pyLDAvis-2.1.2.tar.gz (1.6 MB)\n",
      "Requirement already satisfied: wheel>=0.23.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis==2.1.2) (0.37.0)\n",
      "Requirement already satisfied: numpy>=1.9.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis==2.1.2) (1.20.3)\n",
      "Requirement already satisfied: scipy>=0.18.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis==2.1.2) (1.7.1)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis==2.1.2) (1.3.3)\n",
      "Requirement already satisfied: joblib>=0.8.4 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis==2.1.2) (1.1.0)\n",
      "Requirement already satisfied: jinja2>=2.7.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis==2.1.2) (3.0.2)\n",
      "Requirement already satisfied: numexpr in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis==2.1.2) (2.7.3)\n",
      "Requirement already satisfied: pytest in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis==2.1.2) (6.2.4)\n",
      "Requirement already satisfied: future in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pyLDAvis==2.1.2) (0.18.2)\n",
      "Requirement already satisfied: funcy in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pyLDAvis==2.1.2) (1.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from jinja2>=2.7.2->pyLDAvis==2.1.2) (2.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pandas>=0.17.0->pyLDAvis==2.1.2) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7.3->pandas>=0.17.0->pyLDAvis==2.1.2) (1.16.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (21.2.0)\n",
      "Requirement already satisfied: iniconfig in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.1.1)\n",
      "Requirement already satisfied: packaging in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (21.0)\n",
      "Requirement already satisfied: pluggy<1.0.0a1,>=0.12 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (0.13.1)\n",
      "Requirement already satisfied: py>=1.8.2 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.10.0)\n",
      "Requirement already satisfied: toml in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (0.10.2)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in d:\\anaconda\\anaconda_installation\\lib\\site-packages (from pytest->pyLDAvis==2.1.2) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from pytest->pyLDAvis==2.1.2) (0.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from packaging->pytest->pyLDAvis==2.1.2) (2.4.7)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (setup.py): started\n",
      "  Building wheel for pyLDAvis (setup.py): finished with status 'done'\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97738 sha256=658f587df7d592ba603d5ee1cfe68e7ad834fc581e7539d168701de050f27ffc\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\59\\70\\ee\\99a0df99d8b4a7b87c79640ebee0927c0f6ccff046e9cc2471\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: pyLDAvis\n",
      "  Attempting uninstall: pyLDAvis\n",
      "    Found existing installation: pyLDAvis 3.3.1\n",
      "    Uninstalling pyLDAvis-3.3.1:\n",
      "      Successfully uninstalled pyLDAvis-3.3.1\n",
      "Successfully installed pyLDAvis-2.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pyLDAvis==2.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042fe1c4-53a4-471e-ae10-17125dbd2460",
   "metadata": {},
   "source": [
    "# pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "3bac78fd-7570-40ca-9806-2600b82bb5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Anaconda_installation\\lib\\site-packages\\pyLDAvis\\_prepare.py:246: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  term_ix = topic_terms.unique()\n"
     ]
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import os # new\n",
    "from pathlib import Path\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#df = pd.read_csv(\"Sanctions/chord/housing.csv\")\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('Sanctions/chord/results/CV2_LDAvis_prepared0_'+str(num_topics))# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, 'Sanctions/chord/results/CV2_ldavis_prepared0_'+ str(num_topics) +'.html')\n",
    "    LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace8398a-db41-4f16-bb7b-72fa1379111c",
   "metadata": {},
   "source": [
    "# Running LDA using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e8dc8553-f001-4fb6-803b-38bcbea4cea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "4d4653f9-c054-4aef-901a-c382a8afbf32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.041*\"quarantin\" + 0.029*\"home\" + 0.028*\"furlough\" + 0.027*\"lose\" + 0.023*\"work\" + 0.023*\"abl\" + 0.021*\"time\" + 0.021*\"leav\" + 0.018*\"month\" + 0.017*\"lockdown\"\n",
      "Topic: 1 Word: 0.020*\"peopl\" + 0.019*\"time\" + 0.018*\"work\" + 0.016*\"depress\" + 0.015*\"health\" + 0.015*\"worri\" + 0.015*\"lack\" + 0.014*\"covid\" + 0.013*\"stick\" + 0.013*\"week\"\n",
      "Topic: 2 Word: 0.036*\"home\" + 0.034*\"work\" + 0.023*\"need\" + 0.023*\"time\" + 0.023*\"school\" + 0.020*\"store\" + 0.018*\"go\" + 0.017*\"kid\" + 0.017*\"groceri\" + 0.016*\"social\"\n",
      "Topic: 3 Word: 0.039*\"coronavirus\" + 0.024*\"econom\" + 0.020*\"affect\" + 0.019*\"leav\" + 0.019*\"forc\" + 0.019*\"famili\" + 0.018*\"hous\" + 0.018*\"work\" + 0.018*\"person\" + 0.018*\"caus\"\n",
      "Topic: 4 Word: 0.047*\"incom\" + 0.026*\"affect\" + 0.022*\"financ\" + 0.021*\"work\" + 0.020*\"home\" + 0.019*\"quarantin\" + 0.018*\"life\" + 0.018*\"covid\" + 0.017*\"isol\" + 0.016*\"hand\"\n",
      "Topic: 5 Word: 0.053*\"work\" + 0.049*\"affect\" + 0.022*\"close\" + 0.021*\"shut\" + 0.020*\"home\" + 0.020*\"countri\" + 0.016*\"univers\" + 0.016*\"current\" + 0.015*\"onlin\" + 0.015*\"financi\"\n",
      "Topic: 6 Word: 0.027*\"limit\" + 0.026*\"life\" + 0.024*\"home\" + 0.024*\"work\" + 0.024*\"friend\" + 0.023*\"affect\" + 0.021*\"reduc\" + 0.020*\"stay\" + 0.019*\"restrict\" + 0.018*\"decreas\"\n",
      "Topic: 7 Word: 0.057*\"home\" + 0.052*\"stay\" + 0.036*\"work\" + 0.023*\"food\" + 0.020*\"famili\" + 0.019*\"relat\" + 0.018*\"stress\" + 0.017*\"caus\" + 0.014*\"bore\" + 0.014*\"high\"\n",
      "Topic: 8 Word: 0.024*\"diseas\" + 0.023*\"work\" + 0.022*\"home\" + 0.020*\"unemploy\" + 0.019*\"feel\" + 0.016*\"plan\" + 0.014*\"famili\" + 0.014*\"close\" + 0.014*\"compani\" + 0.014*\"face\"\n",
      "Topic: 9 Word: 0.026*\"worri\" + 0.026*\"famili\" + 0.023*\"sick\" + 0.023*\"get\" + 0.023*\"virus\" + 0.019*\"affect\" + 0.018*\"work\" + 0.018*\"anxieti\" + 0.016*\"health\" + 0.016*\"corona\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ebff98-3fb2-48de-b3b7-aee78ee7ea78",
   "metadata": {},
   "source": [
    "# Classification of the topics\n",
    "## Performance evaluation by classifying sample document using LDA Bag of Words model\n",
    "\n",
    "Our test document has the highest probability to be part of the topic that our model assigned, which is the accurate classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf77de-7866-437e-b8b6-c7d4fa4753e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_docs[600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afb9069-8240-4c92-8a14-ae1b77164016",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[600]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2eaa738a-ae40-42b4-8abb-2be3ca2ab459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mostly the only real change is that I am working from home and not seeing anyone in person -- I am taking more calls and reaching out to friends via zoom for socialization, and going to \"virtual\" events at times. I also am making an effort to do things that \"fill me up\", like hobbies and exercise and reading. I am cooking all my food at home, and freezing leftovers and produce to avoid having to go to the stores, which are out of many items.'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['CV2'][600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "aff7344e-5e5f-43a5-a3ad-eb59f55b416b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I could not find a job now because everything is closed. I am a student and university is also closed. Just at home until the shut down is over.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents['CV2'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "017e7cf3-d3e6-4c5d-85c1-f27a2d9875bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close', 'student', 'univers', 'close', 'home', 'shut']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c88a703b-ec0b-4853-846c-4a136ac54234",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.8713736534118652\t \n",
      "Topic: 0.072*\"home\" + 0.053*\"work\" + 0.042*\"famili\" + 0.041*\"incom\" + 0.033*\"close\" + 0.027*\"school\" + 0.025*\"quarantin\" + 0.023*\"reduc\" + 0.022*\"stress\" + 0.021*\"econom\"\n",
      "\n",
      "Score: 0.014303410425782204\t \n",
      "Topic: 0.073*\"home\" + 0.062*\"work\" + 0.047*\"affect\" + 0.034*\"limit\" + 0.026*\"stay\" + 0.026*\"life\" + 0.022*\"covid\" + 0.020*\"social\" + 0.020*\"food\" + 0.020*\"peopl\"\n",
      "\n",
      "Score: 0.01429315097630024\t \n",
      "Topic: 0.115*\"work\" + 0.063*\"home\" + 0.022*\"time\" + 0.018*\"care\" + 0.018*\"covid\" + 0.015*\"hour\" + 0.015*\"school\" + 0.015*\"peopl\" + 0.014*\"month\" + 0.014*\"famili\"\n",
      "\n",
      "Score: 0.014292443171143532\t \n",
      "Topic: 0.098*\"work\" + 0.063*\"home\" + 0.023*\"groceri\" + 0.020*\"school\" + 0.019*\"time\" + 0.018*\"shop\" + 0.017*\"quarantin\" + 0.015*\"close\" + 0.015*\"week\" + 0.015*\"money\"\n",
      "\n",
      "Score: 0.014292404986917973\t \n",
      "Topic: 0.083*\"work\" + 0.051*\"home\" + 0.047*\"affect\" + 0.022*\"go\" + 0.018*\"health\" + 0.017*\"school\" + 0.016*\"wife\" + 0.016*\"life\" + 0.016*\"covid\" + 0.015*\"social\"\n",
      "\n",
      "Score: 0.014292179606854916\t \n",
      "Topic: 0.061*\"work\" + 0.042*\"home\" + 0.033*\"get\" + 0.032*\"like\" + 0.023*\"furlough\" + 0.021*\"live\" + 0.021*\"sick\" + 0.020*\"worri\" + 0.020*\"current\" + 0.019*\"go\"\n",
      "\n",
      "Score: 0.014288525097072124\t \n",
      "Topic: 0.069*\"famili\" + 0.032*\"worri\" + 0.031*\"friend\" + 0.028*\"stress\" + 0.027*\"hand\" + 0.026*\"sick\" + 0.023*\"home\" + 0.021*\"get\" + 0.020*\"meet\" + 0.020*\"virus\"\n",
      "\n",
      "Score: 0.014288345351815224\t \n",
      "Topic: 0.079*\"home\" + 0.068*\"work\" + 0.053*\"time\" + 0.022*\"week\" + 0.021*\"person\" + 0.020*\"groceri\" + 0.018*\"hous\" + 0.018*\"life\" + 0.016*\"stay\" + 0.016*\"virus\"\n",
      "\n",
      "Score: 0.014288344420492649\t \n",
      "Topic: 0.044*\"work\" + 0.041*\"home\" + 0.039*\"famili\" + 0.031*\"feel\" + 0.028*\"stay\" + 0.027*\"affect\" + 0.021*\"hous\" + 0.017*\"friend\" + 0.017*\"peopl\" + 0.016*\"get\"\n",
      "\n",
      "Score: 0.014287577010691166\t \n",
      "Topic: 0.069*\"home\" + 0.052*\"work\" + 0.034*\"worri\" + 0.028*\"cancel\" + 0.021*\"plan\" + 0.020*\"stay\" + 0.018*\"affect\" + 0.018*\"trip\" + 0.017*\"bore\" + 0.016*\"infect\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[1]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024a080c-f4db-4217-814a-ff0eab44ca8b",
   "metadata": {},
   "source": [
    "# Performance evaluation by classifying sample document using LDA TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f45250d0-596a-4d6e-a7da-e62bf90d7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.7749688029289246\t \n",
      "Topic: 0.066*\"work\" + 0.051*\"home\" + 0.023*\"famili\" + 0.022*\"time\" + 0.021*\"social\" + 0.019*\"school\" + 0.018*\"abl\" + 0.017*\"quarantin\" + 0.015*\"month\" + 0.015*\"close\"\n",
      "\n",
      "Score: 0.025008346885442734\t \n",
      "Topic: 0.034*\"life\" + 0.028*\"caus\" + 0.025*\"covid\" + 0.022*\"affect\" + 0.020*\"financ\" + 0.019*\"money\" + 0.018*\"furlough\" + 0.016*\"person\" + 0.016*\"work\" + 0.015*\"place\"\n",
      "\n",
      "Score: 0.025004662573337555\t \n",
      "Topic: 0.038*\"stay\" + 0.034*\"home\" + 0.030*\"feel\" + 0.023*\"work\" + 0.022*\"time\" + 0.021*\"current\" + 0.019*\"week\" + 0.016*\"stick\" + 0.016*\"school\" + 0.016*\"anxieti\"\n",
      "\n",
      "Score: 0.025003833696246147\t \n",
      "Topic: 0.019*\"get\" + 0.018*\"worri\" + 0.018*\"hous\" + 0.018*\"work\" + 0.018*\"outsid\" + 0.016*\"coronavirus\" + 0.016*\"affect\" + 0.016*\"peopl\" + 0.015*\"go\" + 0.015*\"famili\"\n",
      "\n",
      "Score: 0.02500378154218197\t \n",
      "Topic: 0.032*\"home\" + 0.028*\"work\" + 0.024*\"stay\" + 0.020*\"friend\" + 0.018*\"groceri\" + 0.017*\"need\" + 0.017*\"famili\" + 0.016*\"relat\" + 0.016*\"peopl\" + 0.016*\"affect\"\n",
      "\n",
      "Score: 0.025003550574183464\t \n",
      "Topic: 0.049*\"quarantin\" + 0.033*\"home\" + 0.026*\"work\" + 0.023*\"close\" + 0.023*\"chang\" + 0.020*\"pandem\" + 0.020*\"school\" + 0.018*\"life\" + 0.018*\"incom\" + 0.018*\"longer\"\n",
      "\n",
      "Score: 0.025002771988511086\t \n",
      "Topic: 0.055*\"affect\" + 0.050*\"incom\" + 0.031*\"reduc\" + 0.030*\"face\" + 0.025*\"work\" + 0.024*\"decreas\" + 0.021*\"econom\" + 0.019*\"friend\" + 0.019*\"live\" + 0.016*\"issu\"\n",
      "\n",
      "Score: 0.02500222623348236\t \n",
      "Topic: 0.031*\"limit\" + 0.025*\"famili\" + 0.024*\"work\" + 0.023*\"health\" + 0.020*\"home\" + 0.020*\"worri\" + 0.018*\"access\" + 0.017*\"peopl\" + 0.017*\"like\" + 0.017*\"affect\"\n",
      "\n",
      "Score: 0.02500130422413349\t \n",
      "Topic: 0.026*\"affect\" + 0.024*\"lose\" + 0.024*\"home\" + 0.021*\"work\" + 0.020*\"impact\" + 0.020*\"person\" + 0.016*\"hand\" + 0.016*\"coronavirus\" + 0.015*\"leav\" + 0.013*\"econom\"\n",
      "\n",
      "Score: 0.025000760331749916\t \n",
      "Topic: 0.029*\"stress\" + 0.025*\"famili\" + 0.022*\"worri\" + 0.022*\"get\" + 0.022*\"bore\" + 0.022*\"effect\" + 0.021*\"risk\" + 0.021*\"isol\" + 0.020*\"love\" + 0.020*\"activ\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model_tfidf[bow_corpus[200]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e9229-405e-4167-8ae6-d1b4172f6371",
   "metadata": {},
   "source": [
    "# Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c5aebdd0-55a9-4e57-acdf-ca68d1ddc402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.7162970900535583\t Topic: 0.072*\"home\" + 0.053*\"work\" + 0.042*\"famili\" + 0.041*\"incom\" + 0.033*\"close\"\n",
      "Score: 0.23035505414009094\t Topic: 0.098*\"work\" + 0.063*\"home\" + 0.023*\"groceri\" + 0.020*\"school\" + 0.019*\"time\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"\"\"I strictly adhere to health regulations. I sleep less at night because I'm afraid that it will happen to me or one of my family members. I am much more irritable and nervous. I started training at home. I haven't been out for about 35 days. Covid 19 has stopped the renovations in our new home. I don't have free time because my children are small and the kindergartens are closed.\"\"\"\n",
    "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdddd034-342f-4548-a5cd-4e99ac89462e",
   "metadata": {},
   "source": [
    "# CV2 column LDAvis topic visualization\n",
    "\n",
    "LDAviz tool helps visualization of the multi-dimensional data into 2 dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a524fd37-4b63-4dd0-8050-f6cd886a79eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import os # new\n",
    "from pathlib import Path\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#df = pd.read_csv(\"Sanctions/chord/housing.csv\")\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('Sanctions/chord/results/LDAvis_prepared_'+str(num_topics))# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, 'Sanctions/chord/results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "    LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d253720-b389-46cd-a062-9c3fd285e85f",
   "metadata": {},
   "source": [
    "# Reading the LDAvis interactive topic-modeling data visualization \n",
    "\n",
    "- The size of the Circle on the left describes the importance of that topic in the model/corpus.\n",
    "- You can hover over the circle representing one specific topic on the left to see the words change on the graph to the right related to the specific topic circle you selected.\n",
    "- When you highlight the words on the right (eg: home, stay, etc.,) in the graph, the same sort of areas overall of the topic space gets highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189ac231-5f76-48d0-8630-4d79d3d2da9d",
   "metadata": {},
   "source": [
    "# Limitation of LDAVis\n",
    "\n",
    "- Its really difficult to determine the relative sizes of the circles.(like areas the areas of the circles 2 times, 5 times or 10 times bigger/smaller than each other).\n",
    "- The X and Y axis do not matter. They do not give any information and are meaningless. Note that LDAvis is a projection of the multi-dimensional space into two dimensions\n",
    "- The size and location of the circles are not related to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff026553-0458-414a-be60-dedcb47a9297",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c83c9a-05f8-4beb-8ff7-0a98f6f9b61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2416778b-40c4-4609-8d5c-283f96b9e54e",
   "metadata": {},
   "source": [
    "# SANC2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ff7d0e4a-e5fc-4005-b690-660b6c84b255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Severe inflation has ruined our lives'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data from csv\n",
    "df = pd.read_csv(\"Sanctions/chord/highlighted_sentiments_survey.csv\")\n",
    "df['SANC2'][925] # before 925 are empty (NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4adfd124-664d-48ce-b69a-a66ee3a92b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_20668/2574139200.py:3: DeprecationWarning: invalid escape sequence \\.\n",
      "  df['SANC2'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert the titles to lowercase\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1145    lack of availability of modern technologies\\nl...\n",
       "1146    rising prices of consumer goods that have impo...\n",
       "1147    impossibility of foreign marketing weakness in...\n",
       "1148                       sanctions reduce opportunities\n",
       "1149    lack of access to health and pharmaceutical ma...\n",
       "Name: SANC2_text_processed, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['SANC2_text_processed'] = \\\n",
    "df['SANC2'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert the titles to lowercase\n",
    "df['SANC2_text_processed'] = \\\n",
    "df['SANC2_text_processed'].map(lambda x: x.lower())# Print out the first rows of papers\n",
    "df['SANC2_text_processed'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c29d06a9-7ee9-45c6-bf55-88b6bb69393c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                                  #NULL!\n",
       "1                                                  #NULL!\n",
       "2                                                  #NULL!\n",
       "3                                                  #NULL!\n",
       "4                                                  #NULL!\n",
       "                              ...                        \n",
       "1145    Lack of availability of modern technologies\\nL...\n",
       "1146    Rising prices of consumer goods that have impo...\n",
       "1147    Impossibility of foreign marketing, weakness i...\n",
       "1148                       Sanctions reduce opportunities\n",
       "1149    Lack of access to health and pharmaceutical ma...\n",
       "Name: SANC2, Length: 1150, dtype: object"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['SANC2'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "62cc4397-39a2-43c5-bca2-3e0eed7f139f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "texts = data_words# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "88857331-6c08-431b-bd41-a3c86da4ba06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.027*\"sanctions\" + 0.017*\"economic\" + 0.014*\"products\" + 0.014*\"decrease\" '\n",
      "  '+ 0.013*\"null\" + 0.011*\"affected\" + 0.010*\"impact\" + 0.010*\"lives\" + '\n",
      "  '0.010*\"food\" + 0.008*\"work\"'),\n",
      " (1,\n",
      "  '0.021*\"sanctions\" + 0.021*\"purchasing\" + 0.021*\"power\" + 0.017*\"drug\" + '\n",
      "  '0.016*\"decrease\" + 0.013*\"lack\" + 0.010*\"economic\" + 0.010*\"null\" + '\n",
      "  '0.010*\"reduction\" + 0.010*\"difficulty\"'),\n",
      " (2,\n",
      "  '0.348*\"null\" + 0.027*\"sanctions\" + 0.011*\"increase\" + 0.010*\"financial\" + '\n",
      "  '0.008*\"prices\" + 0.008*\"price\" + 0.008*\"exchange\" + 0.007*\"economic\" + '\n",
      "  '0.007*\"rising\" + 0.006*\"expenses\"'),\n",
      " (3,\n",
      "  '0.024*\"goods\" + 0.020*\"economic\" + 0.019*\"sanctions\" + 0.011*\"lack\" + '\n",
      "  '0.010*\"problems\" + 0.010*\"countries\" + 0.010*\"expensive\" + 0.009*\"people\" + '\n",
      "  '0.009*\"impossibility\" + 0.009*\"raw\"'),\n",
      " (4,\n",
      "  '0.210*\"null\" + 0.014*\"sanctions\" + 0.013*\"impact\" + 0.012*\"goods\" + '\n",
      "  '0.011*\"international\" + 0.010*\"prices\" + 0.010*\"foreign\" + 0.010*\"biggest\" '\n",
      "  '+ 0.009*\"inflation\" + 0.009*\"companies\"'),\n",
      " (5,\n",
      "  '0.328*\"null\" + 0.016*\"income\" + 0.013*\"items\" + 0.011*\"country\" + '\n",
      "  '0.009*\"costs\" + 0.009*\"access\" + 0.009*\"increase\" + 0.009*\"affected\" + '\n",
      "  '0.008*\"exchange\" + 0.008*\"sanctions\"'),\n",
      " (6,\n",
      "  '0.028*\"sanctions\" + 0.025*\"increase\" + 0.013*\"goods\" + 0.013*\"prices\" + '\n",
      "  '0.011*\"currency\" + 0.011*\"inflation\" + 0.010*\"resources\" + 0.010*\"access\" + '\n",
      "  '0.009*\"economic\" + 0.009*\"rising\"'),\n",
      " (7,\n",
      "  '0.075*\"null\" + 0.021*\"due\" + 0.020*\"access\" + 0.018*\"sanctions\" + '\n",
      "  '0.016*\"materials\" + 0.016*\"lack\" + 0.016*\"limited\" + 0.012*\"economic\" + '\n",
      "  '0.010*\"currency\" + 0.010*\"problems\"'),\n",
      " (8,\n",
      "  '0.240*\"null\" + 0.019*\"sanctions\" + 0.013*\"affected\" + 0.011*\"due\" + '\n",
      "  '0.011*\"increase\" + 0.010*\"caused\" + 0.010*\"people\" + 0.009*\"income\" + '\n",
      "  '0.008*\"goods\" + 0.006*\"cost\"'),\n",
      " (9,\n",
      "  '0.445*\"null\" + 0.018*\"sanctions\" + 0.008*\"country\" + 0.006*\"income\" + '\n",
      "  '0.005*\"affected\" + 0.005*\"people\" + 0.005*\"access\" + 0.004*\"due\" + '\n",
      "  '0.004*\"lives\" + 0.004*\"goods\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint# number of topics\n",
    "num_topics = 10# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "8765fc5f-9591-4997-b80a-6bfedcf62af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "#https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import os # new\n",
    "from pathlib import Path\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#df = pd.read_csv(\"Sanctions/chord/housing.csv\")\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('Sanctions/chord/SANC2_topics/SANC2_LDAvis_prepared_0'+str(num_topics))# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, 'Sanctions/chord/SANC2_topics/SANC2_LDAvis_prepared_0'+ str(num_topics) +'.html')\n",
    "    LDAvis_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752dfde-6179-4d64-b373-4577d245f37f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6404410-d3e4-436e-bfaf-a5708efe170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e1ad3-ca4c-4224-b830-f8b01c2e6b9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e20114d1-42e5-48b7-b275-fba54064426d",
   "metadata": {},
   "source": [
    "# CV2 without stemming   using 2nd method like SANC2 topic modeling (gives error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "dfc6e411-3b61-490a-b59d-9c83a8d9f518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It may make it harder to sign a new contract unless a direct or indirect service is provided, otherwise the rest of the time will be marginalized for a long time.'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reading data from csv\n",
    "df = pd.read_csv(\"Sanctions/chord/highlighted_sentiments_survey.csv\")\n",
    "df['CV2'][925] # before 925 are empty (NULL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "9741b2ec-396f-4faf-ab60-a687cf43d652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Store i was working in had to close due the re...\n",
       "1       I could not find a job now because everything ...\n",
       "2       Lectures are cancelled and exams postponed, it...\n",
       "3       During the last month I am not working in a bi...\n",
       "4       I have not been able to go to work which made ...\n",
       "                              ...                        \n",
       "1145    It had positive effects\\nEveryone was paying a...\n",
       "1146    Lack of simple entertainment and gathering tha...\n",
       "1147                         Almost the same as the above\n",
       "1148                                       Overall useful\n",
       "1149    Closing the kindergarten and as a result quitt...\n",
       "Name: CV2, Length: 1150, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['CV2'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1a4f133e-a196-4c0f-a140-8ae9aa9e1aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Store i was working in had to close due the re...\n",
       "1       I could not find a job now because everything ...\n",
       "2       Lectures are cancelled and exams postponed, it...\n",
       "3       During the last month I am not working in a bi...\n",
       "4       I have not been able to go to work which made ...\n",
       "                              ...                        \n",
       "1145    It had positive effects\\nEveryone was paying a...\n",
       "1146    Lack of simple entertainment and gathering tha...\n",
       "1147                         Almost the same as the above\n",
       "1148                                       Overall useful\n",
       "1149    Closing the kindergarten and as a result quitt...\n",
       "Name: CV2, Length: 1150, dtype: object"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['CV2'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e3afbfd9-c95d-48f6-a412-0ff5e4880ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "<>:3: DeprecationWarning: invalid escape sequence \\.\n",
      "C:\\Users\\User\\AppData\\Local\\Temp/ipykernel_20668/4190666080.py:3: DeprecationWarning: invalid escape sequence \\.\n",
      "  df['CV2'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert the titles to lowercase\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20668/4190666080.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove punctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[,\\.!?]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Convert the titles to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Print out the first rows of papers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   4159\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4160\u001b[0m         \"\"\"\n\u001b[1;32m-> 4161\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4162\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   4163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20668/4190666080.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Remove punctuation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[,\\.!?]'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Convert the titles to lowercase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CV2_text_processed'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m# Print out the first rows of papers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\Anaconda_installation\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 210\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "df['CV2_text_processed'] = \\\n",
    "df['CV2'].map(lambda x: re.sub('[,\\.!?]', '', x))# Convert the titles to lowercase\n",
    "df['CV2_text_processed'] = \\\n",
    "df['CV2_text_processed'].map(lambda x: x.lower())# Print out the first rows of papers\n",
    "df['CV2_text_processed'].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92de5f46-242f-4cb6-826a-9e8f7a1a2ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['CV2'].fillna('').astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274e778-a739-4e21-b031-864f7f2720cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)# Create Corpus\n",
    "texts = data_words# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]# View\n",
    "print(corpus[:1][0][:30])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313d2ec7-80e2-4669-acd5-ca354ef799a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint# number of topics\n",
    "num_topics = 10# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b134e-93e2-43f8-ba38-540b5b24d8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim_models as gensimvis\n",
    "#https://stackoverflow.com/questions/66759852/no-module-named-pyldavis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import os # new\n",
    "from pathlib import Path\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "#df = pd.read_csv(\"Sanctions/chord/housing.csv\")\n",
    "\n",
    "LDAvis_data_filepath = os.path.join('Sanctions/chord/results/CV2_topics/CV2_LDAvis_prepared_000'+str(num_topics))# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "    pyLDAvis.save_html(LDAvis_prepared, 'Sanctions/chord/results/CV2_topics/CV2_LDAvis_prepared_000'+ str(num_topics) +'.html')\n",
    "    LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
